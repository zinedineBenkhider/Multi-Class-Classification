{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# TP 7: Classification multi-classe"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\n#import seaborn as sns; sns.set()\nimport numpy as np\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.metrics import *",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Importer le jeu de données `digits`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import datasets",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Charger le jeu de données `digits` via la fonction `datasets.load_digits` dans un nom `digits`. C'est un objet `Bunch` qui ressemble à un dictionnaire. Vous pouvez afficher les clefs d'indice avec `digits.keys()` et regardez la description de ce jeu de données en affichant par `print` le contenu associé à la clef `DESCR` (grâce à `digits['DESCR']` ou de façon équivalente via `digits.DESCR`)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "digits = datasets.load_digits()\nprint(digits.DESCR)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Optical Recognition of Handwritten Digits Data Set\n===================================================\n\nNotes\n-----\nData Set Characteristics:\n    :Number of Instances: 5620\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\nReferences\n----------\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question** Nommez `X_digits` et `y_digits` les valeurs associées aux clefs d'indice `data` et `target` qui correspondent aux données et aux valeur associées (les classes ou cibles)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_digits = digits.data\ny_digits = digits.target\ny_digits, X_digits.shape",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "(array([0, 1, 2, ..., 8, 9, 8]), (1797, 64))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "La clef \"images\" est une liste d'images que l'on peut afficher. Voici les 50 premières :"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nrows, ncols = 5, 10\nfig, ax = plt.subplots(nrows, ncols, figsize=(10, 5))\nfor i in range(nrows*ncols):\n    ax[i//ncols,i%ncols].imshow(digits.images[i], cmap=plt.cm.gray_r)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question**: Découpez l'échantillon en `X_train`, `y_train`, `X_test`, `y_test` en gardant les 100 derniers exemples pour le test."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, test_size=100, random_state=1)\n\n\n",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Vérifiez que vous avez bien les dimensions suivantes pour ces objets : ((1697, 64), (100, 64), (1697,), (100,))\n\n**Question** affichez les dimensions des matrices correspondant aux 4 échantillons."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "((len(X_train),len(X_train[0])),(len(X_test),len(X_test[0])),(len(y_train),),(len(y_test),))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "((1697, 64), (100, 64), (1697,), (100,))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Perceptron multiclasse \"Un-contre-Tous\""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question**: Observez le nombre de classes et etudier leur distribution dans `train` et dans le `test`. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "classes_list = np.unique(y_digits).astype(int)\nprint(\"Nombre de classe:\",len(classes_list),\"\\n\")\n#nombre de classe est 10\nXi_mean_train = [np.mean(X_train[y_train == cls]) for cls in classes_list]\nXi_var_train = [np.var(X_train[y_train == cls]) for cls in classes_list]\nprint(\"#### Train ####\\n\")\nprint(\"Moyenne des x pour chaque classe (par ordre) :\",Xi_mean_train,\"\\n\")\nprint(\"Variance des x pour chaque classe (par ordre) :\",Xi_mean_train)\nXi_mean_test = [np.mean(X_train[y_train == cls], axis=0) for cls in classes_list]\nXi_var_test = [np.var(X_train[y_train == cls], axis=0) for cls in classes_list]\nprint(\"\\n#### Test ####\\n\")\nprint(\"Moyenne des x pour chaque classe (par ordre) :\",Xi_mean_train,\"\\n\")\nprint(\"Variance des x pour chaque classe (par ordre) :\",Xi_mean_train)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Nombre de classe: 10 \n\n#### Train ####\n\nMoyenne des x pour chaque classe (par ordre) : [4.950785928143713, 4.872441520467836, 4.899242424242424, 4.791061046511628, 4.865477071005917, 4.807839912280702, 4.869703389830509, 4.75217803030303, 5.145425451807229, 4.894306752873563] \n\nVariance des x pour chaque classe (par ordre) : [4.950785928143713, 4.872441520467836, 4.899242424242424, 4.791061046511628, 4.865477071005917, 4.807839912280702, 4.869703389830509, 4.75217803030303, 5.145425451807229, 4.894306752873563]\n\n#### Test ####\n\nMoyenne des x pour chaque classe (par ordre) : [4.950785928143713, 4.872441520467836, 4.899242424242424, 4.791061046511628, 4.865477071005917, 4.807839912280702, 4.869703389830509, 4.75217803030303, 5.145425451807229, 4.894306752873563] \n\nVariance des x pour chaque classe (par ordre) : [4.950785928143713, 4.872441520467836, 4.899242424242424, 4.791061046511628, 4.865477071005917, 4.807839912280702, 4.869703389830509, 4.75217803030303, 5.145425451807229, 4.894306752873563]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Nous sommes donc face à un problème **multiclasse**, ou le nombre de classes est *k=10*. Une technique pour traiter ce type de problèmes consiste à le reduire en *k* problemes binaires. Specifiquement, on construit un classifieur binaire pour chaque classe: tous les exemples $x_i$ associes a cette classe recoivent une etiquette $+1$, alors que les exemples des autres classes sont etiquettes $-1$. Ceci correspond a un codage binaire de chaque classe: elle est associee a un vecteur $z$ de taille $k$, tel que $z_i = +1$ if $y_i = k$ et $z_i = -1$ autrement. \n\nCette technique s'appelle le **one-versus-all**. \n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Codage binaire des labels\n\n**Question**:  Concretement, on cree donc un vecteur binaire \"cible\" $y\\in\\{−1,+1\\}^N$ (ou $N$ est le nombre d'exemples) que l'on associe a chaque classe (via un dictionnaire) pour chaque jeu de donnees `train` et `test`. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "donnees=[y_train,y_test]\nassoc=[\"train\",\"test\"]\ndic={}\nres={}\n\nfor d in donnees:#pour chaque vecteur y (y_train,y_test)\n    dic={}\n    for k in range (len(classes_list)):#pour chaque classe\n        z=[]\n        for y in range(len(d)):#pour chaque yi de (y_train, y_test)\n            if k==d[y]:# tester si la valeur de yi est egale a k \n                z=z+[1]\n            else:\n                z=z+[-1]\n    \n        dic[k]=z #Ajouter le vecteur z a la table de hashage  (classe:z)\n    res[assoc[donnees.index(d)]]=dic # Hashé le nom de jeu de donnees avec le dictionnaire {classe:vecteur}\n\n    ",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:17: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question**: On entraine ensuite $10$ classifieurs binaires, dans ce cas-ci un perceptron, que l'on associe chacune des classes. Utilisez l'implementation du perceptron fournie dans `sklearn`. Fixez le nombre d'iterations maximum a $1$, $10$, puis $100$).  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "classePerceprton={}\nfor c in range(len(classes_list)):\n    perc=Perceptron(max_iter=100)\n    perc.fit(X_train,res[\"train\"][c])\n    classePerceprton[c]=perc",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Prediction multiclasse"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question**: On cherche a present a predire une des $k$ classes, en utilisant la regle de decision suivante, ou $\\bf{w}_y$ est le vecteur de parametres appris avec le perceptron binaire pour la classe $y$:\n\n$$\n\\hat{y} = \\text{argmax}_{y\\in\\{1,\\ldots,k\\}} \\bf{w}_y^{\\top} \\bf{x}\n$$\nPredisez les etiquettes multiclasses successivement sur les donnees `train` et `test`. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#trouver le y qui donne la valeur maximale pour W𝐲 ⊤ 𝐱 (pour chaque x)\ndef predict_labels(X):\n    y_hat=[]\n    for i in range (len(X)):\n        max=sum(X[i]*classePerceprton[0].coef_[0])\n        y=0\n        for j in range(1,len(classes_list)):\n            if sum(X[i]*classePerceprton[j].coef_[0])>max:\n                max=sum(X[i]*classePerceprton[j].coef_[0])\n                y=j\n        y_hat.append(y)\n    return y_hat\n\ny_hat_train=predict_labels(X_train)\ny_hat_test=predict_labels(X_test)",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Evaluation "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question:** Calculer le taux d'erreur (cad, nombre de prediction correctes/nombre d'exemples) sur `train` et `test`. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "error=list((y_hat_train==y_train)).count(True)/len(X_train)\nprint(\" Taux d'erreur pour train :\",error)\n\nerror=list((y_hat_test==y_test)).count(True)/len(X_test)\nprint(\" Taux d'erreur pour test :\",error)\n",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": " Taux d'erreur pour train : 0.9840895698291102\n Taux d'erreur pour test : 0.97\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question**: Evaluer egalement votre modele de classification multi-classe en termes de **precision (P)**, **rappel (R)** et **F1-score (F1)**:\n\n$$\nP = \\frac{TP}{TP+FP} \\\\\nR = \\frac{TP}{TP+FN}\\\\\nF1 = 2 \\frac{P\\times R}{P+R}\n$$\nou TP designent les vrais positifs, FP les faux positifs, et FN les faux negatifs. Ces mesures sont fournies dans `sklearn.metrics`. Utiliser l'option `average='macro'`.\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"precision (P):\",precision_score(y_test, y_hat_test, average='macro'))\nprint(\"rappel (R):\",recall_score(y_test, y_hat_test, average='macro') )\nprint(\"F1-score (F1):\",f1_score(y_test, y_hat_test, average='macro'))",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "precision (P): 0.9765501165501165\nrappel (R): 0.9651515151515152\nF1-score (F1): 0.9696080011942081\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question:** En vue de visualiser les erreurs faites pour le modele, calculer la **matrice de confusion**, egalement disponible dans `sklearn.matrix`. Analyser cette matrice. Quelles sont les erreurs les plus frequentes? Tentez d'expliquer. "
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train, y_hat_train,labels=[0, 1, 2,3,4,5,6,7,8,9])\n\n#Les erreurs les plus frequantes sont souvent sur les classes 8 et 9\n",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "array([[167,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0, 167,   0,   2,   0,   0,   2,   0,   0,   0],\n       [  0,   0, 165,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0, 170,   0,   1,   0,   1,   0,   0],\n       [  0,   1,   0,   0, 167,   0,   0,   0,   1,   0],\n       [  0,   0,   0,   0,   0, 171,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0, 177,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0, 165,   0,   0],\n       [  0,   5,   2,   1,   1,   1,   1,   0, 155,   0],\n       [  0,   0,   0,   0,   1,   0,   0,   2,   5, 166]])"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}